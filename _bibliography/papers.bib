---
---

@inproceedings{liu2025flashstu,
  abbr={CDC},
  title={Flash STU: Fast Spectral Transform Units},
  author={Liu*, Y. Isabel and Nguyen*, Windsor and Devre, Yagiz and Dogariu, Evan and Majumdar, Anirudha and Hazan, Elad},
  booktitle={IEEE Conference on Decision and Control},
  year={2025},
  bibtex_show={true},
  selected={true},
  abstract={We propose Flash Spectral Transform Units (STU), a hybrid STU-Attention model that achieves state-of-the-art performance in language modeling, outperforming Transformer and other leading state space models such as Mamba-2 and Mamba-Attention hybrid. We optimize tensordot approximation on the convolution operation of k fixed spectral filters that reduced computational complexity by a factor of k, and analyze STU's optimization behaviors and convex parameterizations on diverse sequence prediction tasks.},
  annotation={* Equal contribution}
}

@article{liu2025slap,
  abbr={Preprint},
  title={SLAP: Shortcut Learning for Abstract Planning},
  author={Liu, Y. Isabel and Li, Bowen and Eysenbach, Benjamin and Silver, Tom},
  journal={Under Review for ICLR 2026},
  year={2025},
  bibtex_show={true},
  selected={true},
  abstract={We design a generic and automatic algorithm, SLAP, to improve any existing task and motion planner (TAMP) by discovering new physical improvisations through model-free learning approach to achieve shorter execution time and adaptive ability to deal with more complex dynamics and multiple objects. We conduct extensive experiments over four PyBullet and one 2D TAMP environments designed from scratch, and investigate SLAP's robustness in environments that go beyond standard TAMP assumptions.}
}
